// AI Agent Service
// ‡πÉ‡∏ä‡πâ AI ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏Å‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö, validate

const aiService = require('./aiService');
const scrapingService = require('./scrapingService');
const { db, pool } = require('../database');
const { v4: uuidv4 } = require('uuid');

class AgentService {
  // Scrape website and extract plant data using AI
  async scrapeWebsite(websiteId, url) {
    try {
      console.log(`ü§ñ AI Agent: Starting scrape for ${url}`);
      
      // 1. Scrape HTML content
      const scrapeResult = await scrapingService.scrapeHTML(url);
      if (!scrapeResult.success) {
        throw new Error(`Failed to scrape: ${scrapeResult.error}`);
      }

      // 2. Extract text content
      const textResult = await scrapingService.extractText(scrapeResult.html);
      if (!textResult.success) {
        throw new Error(`Failed to extract text: ${textResult.error}`);
      }

      // 3. Extract structured data (basic)
      const structuredResult = await scrapingService.extractStructuredData(scrapeResult.html);
      
      // 4. Use AI to analyze and extract plant data
      // Check if it's Facebook
      const isFacebook = url.includes('facebook.com') || url.includes('fb.com');
      const sourceType = isFacebook ? 'Facebook Group/Page' : '‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå';
      
      const aiPrompt = `‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Agent ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏Å‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏≤‡∏Å${sourceType}

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å${sourceType}:
URL: ${url}
Title: ${structuredResult.data?.title || 'N/A'}
Text Content: ${textResult.text.substring(0, 8000)}... (truncated)

${isFacebook ? '‚ö†Ô∏è ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å Facebook Group/Page ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏õ‡∏Å‡∏ï‡∏¥ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤' : ''}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô JSON format ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ô‡∏µ‡πâ:

{
  "supplier": {
    "name": "‡∏ä‡∏∑‡πà‡∏≠‡∏£‡πâ‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤",
    "location": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà",
    "phone": "‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£",
    "website": "${url}"
  },
  "plants": [
    {
      "name": "‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ",
      "scientificName": "‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)",
      "category": "‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà (‡πÑ‡∏°‡πâ‡∏õ‡∏£‡∏∞‡∏î‡∏±‡∏ö, ‡πÑ‡∏°‡πâ‡∏•‡πâ‡∏≠‡∏°, ‡πÑ‡∏°‡πâ‡∏î‡∏≠‡∏Å, ‡πÑ‡∏°‡πâ‡πÉ‡∏ö, ‡πÅ‡∏Ñ‡∏Ñ‡∏ï‡∏±‡∏™, ‡∏ö‡∏≠‡∏ô‡πÑ‡∏ã, ‡∏Å‡∏•‡πâ‡∏ß‡∏¢‡πÑ‡∏°‡πâ)",
      "price": ‡∏£‡∏≤‡∏Ñ‡∏≤ (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç),
      "size": "‡∏Ç‡∏ô‡∏≤‡∏î (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)",
      "description": "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)",
      "stockAvailable": true/false (‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ)
    }
  ],
  "confidence": 0.0-1.0 (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
}

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏•‡πâ‡∏ß‡∏ô‡πÜ ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà‡πÇ‡∏Ñ‡πâ‡∏î‡∏ö‡∏•‡πá‡∏≠‡∏Å (‡πÄ‡∏ä‡πà‡∏ô code fences) ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏≠‡∏∑‡πà‡∏ô‡πÜ`;

      const aiResult = await aiService.analyzeText(aiPrompt);
      
      if (!aiResult || !aiResult.plants) {
        throw new Error('AI failed to extract plant data');
      }

      // 5. Validate and clean data using AI
      const validatedData = await this.validateAndCleanData(aiResult);

      // 6. Save to database
      const savedData = await this.saveScrapingResults(websiteId, url, validatedData);

      return {
        success: true,
        data: savedData,
        rawData: {
          htmlLength: scrapeResult.html.length,
          textLength: textResult.text.length,
          method: scrapeResult.method
        }
      };

    } catch (error) {
      console.error('‚ùå AI Agent Error:', error);
      throw error;
    }
  }

  // Use AI to validate and clean extracted data
  async validateAndCleanData(extractedData) {
    try {
      const validationPrompt = `‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Agent ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡∏∞‡∏°‡∏≤‡πÑ‡∏î‡πâ:
${JSON.stringify(extractedData, null, 2)}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ (‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç)
2. ‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡πÑ‡∏°‡πâ‡∏õ‡∏£‡∏∞‡∏î‡∏±‡∏ö, ‡πÑ‡∏°‡πâ‡∏•‡πâ‡∏≠‡∏°, ‡πÑ‡∏°‡πâ‡∏î‡∏≠‡∏Å, ‡πÑ‡∏°‡πâ‡πÉ‡∏ö, ‡πÅ‡∏Ñ‡∏Ñ‡∏ï‡∏±‡∏™, ‡∏ö‡∏≠‡∏ô‡πÑ‡∏ã, ‡∏Å‡∏•‡πâ‡∏ß‡∏¢‡πÑ‡∏°‡πâ)
3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤ (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•)
4. ‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
5. ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON format ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡πÅ‡∏•‡πâ‡∏ß

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏•‡πâ‡∏ß‡∏ô‡πÜ ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà‡πÇ‡∏Ñ‡πâ‡∏î‡∏ö‡∏•‡πá‡∏≠‡∏Å`;

      const validatedResult = await aiService.analyzeText(validationPrompt);
      
      return validatedResult || extractedData;
    } catch (error) {
      console.error('Validation error:', error);
      return extractedData; // Return original if validation fails
    }
  }

  // Save scraping results to database
  async saveScrapingResults(websiteId, url, data) {
    const jobId = `job_${Date.now()}_${uuidv4()}`;
    
    try {
      // 1. Create scraping job
      await pool.query(`
        INSERT INTO scraping_jobs (id, website_id, url, status, started_at)
        VALUES ($1, $2, $3, $4, NOW())
      `, [jobId, websiteId || null, url, 'processing']);

      // 2. Find or create supplier
      let supplier = null;
      if (data.supplier) {
        supplier = await db.findOrCreateSupplier({
          name: data.supplier.name || '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏',
          location: data.supplier.location || '',
          phone: data.supplier.phone || null,
          phoneNumbers: data.supplier.phone ? [data.supplier.phone] : [],
          description: `Scraped from ${url}`,
          website: data.supplier.website || url
        });
      }

      // 3. Process each plant
      const savedPlants = [];
      if (data.plants && Array.isArray(data.plants)) {
        for (const plantData of data.plants) {
          try {
            // Find or create plant
            const plant = await db.findOrCreatePlant({
              name: plantData.name || '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠',
              category: plantData.category || '‡πÑ‡∏°‡πâ‡∏õ‡∏£‡∏∞‡∏î‡∏±‡∏ö',
              plantType: plantData.category || '‡πÑ‡∏°‡πâ‡∏õ‡∏£‡∏∞‡∏î‡∏±‡∏ö',
              measurementType: plantData.size ? '‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏£‡∏∞‡∏ñ‡∏≤‡∏á' : '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á',
              description: plantData.description || null,
              scientificName: plantData.scientificName || ''
            });

            // Save scraping result
            const resultId = `result_${Date.now()}_${uuidv4()}`;
            await pool.query(`
              INSERT INTO scraping_results (id, job_id, plant_id, supplier_id, plant_name, price, size, raw_data)
              VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            `, [
              resultId,
              jobId,
              plant.id,
              supplier?.id || null,
              plantData.name,
              plantData.price || 0,
              plantData.size || null,
              JSON.stringify(plantData)
            ]);

            // Update plant-supplier relationship if price exists
            if (supplier && plantData.price) {
              await db.upsertPlantSupplier(plant.id, supplier.id, {
                price: plantData.price,
                size: plantData.size || null
              });
            }

            savedPlants.push({
              plantId: plant.id,
              plantName: plant.name,
              price: plantData.price,
              supplierId: supplier?.id
            });
          } catch (error) {
            console.error(`Error saving plant ${plantData.name}:`, error);
          }
        }
      }

      // 4. Update job status
      await pool.query(`
        UPDATE scraping_jobs
        SET status = $1, completed_at = NOW(), result = $2
        WHERE id = $3
      `, ['completed', JSON.stringify({ plants: savedPlants, count: savedPlants.length }), jobId]);

      return {
        jobId,
        supplierId: supplier?.id,
        plants: savedPlants,
        count: savedPlants.length,
        confidence: data.confidence || 0.5
      };

    } catch (error) {
      // Update job status to failed
      await pool.query(`
        UPDATE scraping_jobs
        SET status = $1, completed_at = NOW(), result = $2
        WHERE id = $3
      `, ['failed', JSON.stringify({ error: error.message }), jobId]);
      throw error;
    }
  }
}

module.exports = new AgentService();

